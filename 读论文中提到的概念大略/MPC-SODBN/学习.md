Model predictive control (MPC)模型预测控制<br>
MPC个人理解就是通过现在的数据来预测一段时间内的未来，并以此得出未来一小段时间内（比如几秒内）的最佳控制方案，然后只执行第一时间的控制方案（比如接下来第一秒的），而后再获取数据……循环往复。<br>
<br>
举个简单的例子：<br>
可以把MPC想象成一个不断“瞻前顾后”的司机：<br>
·司机会根据当前车的位置和速度，预测未来几秒钟的行驶路线。<br>
·司机会计算出一套未来几秒内的最佳方向盘调整方案。<br>
·司机只会按照第一秒的方案调整方向盘，然后再观察车的位置和速度，重新预测和计算未来的最佳方案。<br>
<br>
open-loop optimal control problem开环最优控制问题<br>
所谓开环控制与闭环控制，可以这么理解，自动驾驶开汽车，可以根据车速来进行油门刹车的控制，此所谓闭环；但如果是自动射击，那被打死的人可没法告诉枪有没有把人打死，此所谓开环。<br>
之所以叫开环和闭环，来源于电路中的控制器与执行器之间是否有反馈电路，有了反馈电路则成为闭环，反之则为开环。<br>
概括来说，控制器按照预先设定的策略执行控制，而不根据系统的反馈进行调整的，称为开环控制。<br>
而闭环控制，是一种利用系统的实际输出或状态反馈来调整控制输入的控制方式。控制器会根据系统的反馈信息不断调整控制策略，以达到期望的输出。<br>
其中反馈电路就负责提供反馈信息的~<br>
稍加思考，洗衣机洗铁秤砣的时候把自己带着电源线洗崩了的，一定是开源控制啦~<br>
<br>
深度置信网络（DBN）<br>
DBN是一种由多层受限玻尔兹曼机（RBM）堆叠而成的深度生成模型，通过逐层无监督预训练和整体有监督微调。<br>
说实话，看了半天也没搞明白其中与普通神经网络（比如经典的MLP）的区别精髓。也许在于RBM与神经网络中的中间层区别，看上去分的更开一些。（补：扯淡呢，这不是~）<br>
在这里简单说一下，一个RBM是由一个可见层和一个隐藏层构成的。<br>
可见层就是该RBM接收的数据，每一对可见层和隐藏层之间的节点都有连接，这些连接有权重。此外，每个节点还有一个偏置项（是的，不论是可见层还是隐藏层，每个节点都有一个偏置项）。<br>
当多个RBM堆积成一个DBN时，一般上一个RBM的隐藏层将作为下一个RBM的可见层。<br>
2024.9.10：<br>
tmd，我明白了，RBM的连接方式与MLP的区别。。。。<br>
RBM中的连接方式是受限的，每个隐藏层节点只与可见层的一个节点相连，这限制了模型的复杂性。相比之下，全连接层中的连接是全连接的，每个神经元可以与前一层的所有神经元相连，这使得全连接层能够捕捉更复杂的模式。<br>
没错，就是那么简单，之前不懂的时候就是很脑缺<br>
<br>
吉布斯采样近似得到（这时用于DBN的无监督初始化的知识点，虽然不太理解如何进行无监督初始化的，不过先把这个知识点了解一下吧）<br>
举例来说：<br>
1.你闭上眼睛，从袋子里随机抽取一些球（初始化样本）。<br>
2.你每次只看手中的一颗球，并根据手中其他球的颜色来决定是否要更换这颗球。例如，如果你发现手中红色球太多<br>
3.而蓝色球太少，你就可能决定用一颗蓝色球替换手中的红色球。<br>
4.你重复这个过程很多次，每次只更换一颗球，并且每次更换都考虑手中球的颜色分布。<br>
5.最终，你手中球的颜色分布将越来越接近袋子里球的真实颜色分布。<br>
大概就是如此，但专业的来讲：<br>
1.初始化：从某个初始状态开始，这个状态可以是随机的或者基于一些先验知识。<br>
2.迭代采样：在每次迭代中，你按照以下步骤操作：<br>
    ·选择一个变量（在比喻中，就是选择一颗球）。<br>
    ·根据所有其他变量的当前状态（比喻中，就是手中其他球的颜色），计算并采样这个变量的新值。在概率模型中，·这意味着根据其他变量的值来计算当前变量的条件概率分布，并从中抽取一个样本。<br>
    ·更新这个变量的值（在比喻中，就是更换球的颜色）。<br>
3.重复：重复步骤2很多次，每次迭代只更新一个变量的值。<br>
4.收敛：随着迭代次数的增加，样本集将逐渐收敛到目标概率分布。<br>
乍听上去十分神奇的说，为啥这样可以有效呢？<br>
通俗答曰：每次迭代时，虽然只更新一个变量的值，但是由于每个变量的更新都考虑了其他所有变量的当前状态，因此整个系统的状态会逐渐趋于稳定，最终反映目标概率分布的特征。<br>
<br>
RMSE是“均方根误差”（Root Mean Square Error）的缩写。<br>
<br>
