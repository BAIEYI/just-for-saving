### Dropout
Dropout 是一种正则化技术，用于防止神经网络过拟合。它在训练过程中随机“丢弃”（即设置为零）网络中的一些神经元，这意味着在每次迭代中，网络都会以不同的方式学习数据。
#### 与bn层的关系：
虽然 Batch Normalization (BN) 层不能直接替代 Dropout，但它们都是为了提高神经网络的性能和泛化能力而设计的。

Batch Normalization：BN 层通过对每个小批量数据进行归一化来加速训练过程，并允许使用更高的学习率。它通过减少内部协变量偏移（Internal Covariate Shift）来提高训练的稳定性。
与 Dropout 的关系：BN 和 Dropout 可以同时使用，它们有不同的作用。BN 改善了梯度的流动，而 Dropout 通过随机丢弃神经元来减少过拟合。

### GELU激活函数
Gaussian Error Linear Unit（GELU）是一种在深度学习中常用的激活函数，它在许多神经网络模型中表现出色，尤其是在自然语言处理（NLP）领域，例如在 Transformer 架构中。
直接看函数图像吧。
